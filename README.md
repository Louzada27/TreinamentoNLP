## üáßüá∑ Modelos de Classifica√ß√£o de Texto em Portugu√™s com BERTimbau

Este reposit√≥rio cont√©m scripts para treinar e avaliar modelos de classifica√ß√£o de texto em portugu√™s com base na arquitetura **BERTimbau**. O foco est√° em tarefas como:

- ‚úÖ **An√°lise de Sentimento**  
- ‚úÖ **Detec√ß√£o de Discurso de √ìdio**  
- ‚úÖ **Identifica√ß√£o de Linguagem Impr√≥pria**

Todos os modelos desenvolvidos aqui foram aplicados na seguinte aplica√ß√£o pr√°tica:  
üîó [YouTubeSafeKids-Python](https://github.com/Louzada27/YouTubeSafeKids-Python)

---

## üìë Sobre o Projeto

Este projeto fornece um framework completo para realizar **fine-tuning** do modelo [`neuralmind/bert-base-portuguese-cased`](https://huggingface.co/neuralmind/bert-base-portuguese-cased) (BERTimbau) em tarefas de **classifica√ß√£o de texto** no idioma portugu√™s.  
Foram utilizados os seguintes datasets:

- **LexiconPT** (para An√°lise de Sentimento)
- **HateBR** (para Detec√ß√£o de Discurso de √ìdio)
- **OLID-BR** (para Identifica√ß√£o de Linguagem Impr√≥pria)

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.x**
- **PyTorch**: Framework de deep learning
- **Hugging Face Transformers**: Biblioteca para modelos de PLN de √∫ltima gera√ß√£o
- **Hugging Face Datasets**: Biblioteca para carregar e processar datasets
- **Scikit-learn**: Para m√©tricas de avalia√ß√£o e fun√ß√µes utilit√°rias (como pesos de classe)
- **Pandas**: Para manipula√ß√£o de dados
- **NumPy**: Para opera√ß√µes num√©ricas

---

## üìÅ Estrutura do Projeto

.
‚îú‚îÄ‚îÄ Data-set/                     # Diret√≥rio contendo os datasets
‚îÇ   ‚îú‚îÄ‚îÄ HateBR-main/              # Arquivos do dataset HateBR
‚îÇ   ‚îú‚îÄ‚îÄ lexiconPT-master/         # Arquivos do dataset LexiconPT
‚îÇ   ‚îî‚îÄ‚îÄ ...                       # Outros datasets que podem ser adicionados
‚îú‚îÄ‚îÄ train_sentiment.py            # Script para treinar modelo de an√°lise de sentimento (LexiconPT)
‚îú‚îÄ‚îÄ train_hatbr.py                # Script para treinar modelo no dataset HateBR
‚îú‚îÄ‚îÄ train_olid.py                 # Script para treinar modelo no dataset OLID-BR
‚îú‚îÄ‚îÄ train_model.py                # Script gen√©rico para fine-tuning do BERTimbau (potencialmente menos usado agora)
‚îú‚îÄ‚îÄ test_sentimento.py            # Script para testar o modelo de sentimento
‚îú‚îÄ‚îÄ test_hatbr.py                 # Script para testar o modelo HateBR
‚îú‚îÄ‚îÄ test_olidBr.py                # Script para testar o modelo OLID-BR
‚îú‚îÄ‚îÄ README.md                     # Este arquivo
‚îî‚îÄ‚îÄ ...                           # Outros arquivos de configura√ß√£o/utilit√°rios

---

## üöÄ Como Usar

### 1. Configura√ß√£o

a. **Clone o reposit√≥rio:**
    ```bash
    git clone <url-do-repositorio>
    cd <diretorio-do-repositorio>
    ```

b. **Instale as depend√™ncias:** √â recomendado usar um ambiente virtual.
    ```bash
    pip install torch transformers datasets pandas numpy scikit-learn
    ```
    *(Voc√™ pode precisar de vers√µes espec√≠ficas com base na compatibilidade. Verifique os imports nos scripts se ocorrerem erros.)*

c. **Baixe os Datasets:**
- Link para o dataset **HateBR**: https://github.com/franciellevargas/HateBR
- Link para o dataset **LexiconPT**: https://github.com/sillasgonzaga/lexiconPT
- Link para o dataset **OLID-BR**: https://huggingface.co/datasets/dougtrajano/olid-br

--


##  Avaliando Modelos
### 1. Modelo de An√°lise de Sentimento (BERTimbau + LexiconPT)

O modelo de an√°lise de sentimento foi treinado para classificar textos em tr√™s categorias: **Positivo**, **Negativo** e **Neutro**, utilizando o dataset **LexiconPT**.

#### M√©tricas Gerais:
- **Acur√°cia**: 76.84%
- **Precis√£o**: 76.75%
- **Revoca√ß√£o**: 76.84%
- **F1-Score**: 76.63%

#### Matriz de Confus√£o:
| Predito:     | Positivo | Negativo | Neutro |
|--------------|----------|----------|--------|
| **Real: Positivo**  | 1052     | 292      | 360    |
| **Real: Negativo**  | 118      | 2586     | 254    |
| **Real: Neutro**    | 272      | 195      | 1309   |

#### Desempenho por Classe:
- **Positivo**:
  - **Precis√£o**: 72.95%
  - **Revoca√ß√£o**: 61.74%
  - **F1-Score**: 66.88%
- **Negativo**:
  - **Precis√£o**: 84.15%
  - **Revoca√ß√£o**: 87.42%
  - **F1-Score**: 85.76%
- **Neutro**:
  - **Precis√£o**: 68.07%
  - **Revoca√ß√£o**: 73.70%
  - **F1-Score**: 70.78%

---

### 2. Modelo de Classifica√ß√£o de Toxicidade (BERTimbau + HateBR)

Este modelo foi projetado para classificar conte√∫dos como **T√≥xicos** ou **N√£o T√≥xicos**, utilizando o dataset **HateBR**.

#### M√©tricas Gerais:
- **Acur√°cia**: 96.71%
- **Precis√£o**: 96.72%
- **Revoca√ß√£o**: 96.71%
- **F1-Score**: 96.71%

#### Matriz de Confus√£o:
| Predito:      | N√£o T√≥xico | T√≥xico |
|---------------|------------|--------|
| **Real: N√£o T√≥xico** | 3369       | 131    |
| **Real: T√≥xico**    | 99        | 3401   |

#### Desempenho por Classe:
- **N√£o T√≥xico**:
  - **Precis√£o**: 97.15%
  - **Revoca√ß√£o**: 96.26%
  - **F1-Score**: 96.70%
- **T√≥xico**:
  - **Precis√£o**: 96.29%
  - **Revoca√ß√£o**: 97.17%
  - **F1-Score**: 96.73%

---

## 3. Detec√ß√£o de linguagem impropria (BERTimbau + OLYD-BR)


#### M√©tricas Gerais

- **Loss de Avalia√ß√£o**: 0.1867
- **Acur√°cia**: 51.44%
- **F1-Score (M√©dia Ponderada)**: 74.30%
- **Micro F1-Score**: 77.83%
- **Macro F1-Score**: 35.69%
- **Precis√£o (M√©dia Ponderada)**: 76.98%
- **Revoca√ß√£o (M√©dia Ponderada)**: 74.63%
- **Hamming Loss**: 0.0661
- **Jaccard Score (M√©dia Ponderada)**: 63.01%

---

#### M√©tricas por Classe

A seguir, as m√©tricas detalhadas de F1-Score, Precis√£o e Revoca√ß√£o para cada classe.

| Classe                      | F1-Score  | Precis√£o  | Revoca√ß√£o |
|-----------------------------|-----------|-----------|-----------|
| **Sa√∫de**                    | 0.0       | 0.0       | 0.0       |
| **Ideologia**                | 0.6343    | 0.6728    | 0.6000    |
| **Insulto**                  | 0.8996    | 0.8578    | 0.9456    |
| **LGBTQIA+ Fobia**           | 0.4429    | 0.6739    | 0.3298    |
| **Outros Estilos de Vida**   | 0.2000    | 0.6667    | 0.1176    |
| **Aspectos F√≠sicos**         | 0.3301    | 0.8500    | 0.2048    |
| **Profanidade/Obscenidade**  | 0.7730    | 0.7927    | 0.7543    |
| **Racismo**                  | 0.0       | 0.0       | 0.0       |
| **Intoler√¢ncia Religiosa**   | 0.2893    | 0.5227    | 0.2000    |
| **Sexismo**                  | 0.0       | 0.0       | 0.0       |

---

- **Classe "Sa√∫de"** e **"Racismo"** n√£o apresentaram nenhum desempenho, com F1-Score, Precis√£o e Revoca√ß√£o iguais a 0. Isso pode indicar que o modelo n√£o foi capaz de identificar exemplos dessas classes no conjunto de teste.
  
- **Classe "Insulto"** apresentou resultados muito bons, com F1-Score de 0.8996, indicando um bom equil√≠brio entre Precis√£o (85.78%) e Revoca√ß√£o (94.56%).

- **Classe "LGBTQIA+ Fobia"** teve um desempenho menor, com F1-Score de 0.4429, refletindo uma menor capacidade de identificar corretamente os exemplos dessa categoria.

- **Classe "Outros Estilos de Vida"** tamb√©m teve uma pontua√ß√£o baixa de F1-Score (0.2000), com uma alta Precis√£o (66.67%) mas baixa Revoca√ß√£o (11.76%).

- **Classe "Aspectos F√≠sicos"** obteve uma Precis√£o muito boa (85.00%), mas o F1-Score e a Revoca√ß√£o indicam que o modelo teve dificuldades em identificar corretamente os exemplos dessa classe.

- **Classe "Profanidade/Obscenidade"** obteve bons resultados com F1-Score de 0.7730, refletindo boa Precis√£o (79.27%) e Revoca√ß√£o (75.43%).

- **Classe "Intoler√¢ncia Religiosa"** teve um desempenho baixo com F1-Score de 0.2893, devido √† baixa Revoca√ß√£o (20.00%).

- **Classe "Sexismo"** tamb√©m n√£o apresentou desempenho, com F1-Score, Precis√£o e Revoca√ß√£o iguais a 0.

---

Esses resultados indicam √°reas de sucesso e de melhoria para o modelo, sendo necess√°rio um ajuste adicional para classes com desempenho insatisfat√≥rio, como "Sa√∫de", "Racismo" e "Sexismo". A classe "Insulto" foi a mais bem identificada, e outras classes como "Profanidade/Obscenidade" e "Ideologia" apresentam bons resultados, mas ainda podem ser melhoradas.

Devido √† maior import√¢ncia da classe "Profanidade/Obscenidade", essencial para identificar linguagem impr√≥pria, foram aplicadas medidas espec√≠ficas para melhorar sua avalia√ß√£o durante o treinamento. Essas medidas inclu√≠ram o uso de um threshold (limiar) de 70% e a aplica√ß√£o da t√©cnica de Class Weights, com um fator de 3. Isso significou atribuir maior peso a essa classe em rela√ß√£o √†s outras, para que o modelo focasse mais em identificar corretamente os exemplos de profanidade/obscenidade.

Com essa abordagem, foi poss√≠vel aumentar a acur√°cia do modelo em 6%, alcan√ßando o melhor desempenho na categoria de profanidade.

O recall, especificamente, foi beneficiado por essa estrat√©gia. O recall √© uma m√©trica que mede a capacidade do modelo de identificar corretamente todos os exemplos positivos de uma classe (neste caso, a classe "Profanidade/Obscenidade"). Uma maior √™nfase nessa classe, por meio dos ajustes de threshold e Class Weights, permitiu uma maior recupera√ß√£o de exemplos reais de profanidade, garantindo que o modelo identificasse de forma mais eficaz as inst√¢ncias relevantes dessa categoria. Isso foi crucial para melhorar o desempenho do modelo, especialmente em tarefas de modera√ß√£o autom√°tica, onde a precis√£o na detec√ß√£o de linguagem inadequada √© fundamental.


#####
Apos a conclus√£o suba o modelo para o huggie face e use ele nos testes

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes. (Assumindo licen√ßa MIT com base no README anterior)