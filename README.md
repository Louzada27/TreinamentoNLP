# Modelos de ClassificaÃ§Ã£o de Texto em PortuguÃªs com BERTimbau

Este repositÃ³rio contÃ©m scripts para treinar e avaliar vÃ¡rios modelos de classificaÃ§Ã£o de texto em portuguÃªs baseados na arquitetura BERTimbau. O foco estÃ¡ em tarefas como AnÃ¡lise de Sentimento, DetecÃ§Ã£o de Discurso de Ã“dio e, potencialmente, outras tarefas de PLN relacionadas usando datasets especÃ­ficos em portuguÃªs.

## ğŸ“‘ Sobre o Projeto

Este projeto fornece um framework e implementaÃ§Ãµes especÃ­ficas para o fine-tuning do modelo `neuralmind/bert-base-portuguese-cased` (BERTimbau) para tarefas de classificaÃ§Ã£o de texto relevantes para a lÃ­ngua portuguesa. Inclui scripts para treinar modelos em datasets como LexiconPT (para AnÃ¡lise de Sentimento), HateBR e OLID-BR (provavelmente para DetecÃ§Ã£o de Discurso de Ã“dio/Linguagem Ofensiva).

## ğŸ› ï¸ Tecnologias Utilizadas

*   **Python 3.x**
*   **PyTorch**: Framework de deep learning.
*   **Hugging Face Transformers**: Biblioteca para modelos de PLN de Ãºltima geraÃ§Ã£o, usada aqui para o BERTimbau.
*   **Hugging Face Datasets**: Biblioteca para carregar e processar datasets.
*   **Scikit-learn**: Para mÃ©tricas de avaliaÃ§Ã£o e funÃ§Ãµes utilitÃ¡rias (como pesos de classe).
*   **Pandas**: Para manipulaÃ§Ã£o de dados.
*   **NumPy**: Para operaÃ§Ãµes numÃ©ricas.

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ Data-set/                 # DiretÃ³rio contendo os datasets
â”‚   â”œâ”€â”€ HateBR-main/          # Arquivos do dataset HateBR
â”‚   â”œâ”€â”€ lexiconPT-master/     # Arquivos do dataset LexiconPT
â”‚   â””â”€â”€ ...                   # Outros datasets (ex: OLID-BR)
â”œâ”€â”€ train_sentiment.py        # Script para treinar modelo de anÃ¡lise de sentimento (LexiconPT)
â”œâ”€â”€ train_hatbr.py            # Script para treinar modelo no dataset HateBR
â”œâ”€â”€ train_olid.py             # Script para treinar modelo no dataset OLID-BR
â”œâ”€â”€ train_model.py            # Script genÃ©rico para fine-tuning do BERTimbau (potencialmente menos usado agora)
â”œâ”€â”€ test_sentimento.py        # Script para testar o modelo de sentimento
â”œâ”€â”€ test_hatbr.py             # Script para testar o modelo HateBR
â”œâ”€â”€ test_OlydBr.py            # Script para testar o modelo OLID-BR
â”œâ”€â”€ README.md                 # Este arquivo
â””â”€â”€ ...                       # Outros arquivos de configuraÃ§Ã£o/utilitÃ¡rios
```
*(Nota: A estrutura do dataset pode variar)*

## ğŸš€ Como Usar

### 1. ConfiguraÃ§Ã£o

a.  **Clone o repositÃ³rio:**
    ```bash
    git clone <url-do-repositorio>
    cd <diretorio-do-repositorio>
    ```

b.  **Instale as dependÃªncias:** Ã‰ recomendado usar um ambiente virtual.
    ```bash
    pip install torch transformers datasets pandas numpy scikit-learn
    ```
    *(VocÃª pode precisar de versÃµes especÃ­ficas com base na compatibilidade. Verifique os imports nos scripts se ocorrerem erros.)*

c.  **Baixe os Datasets:** Garanta que os datasets necessÃ¡rios (LexiconPT, HateBR, OLID-BR) sejam baixados e colocados corretamente na estrutura do diretÃ³rio `Data-set/` conforme esperado pelos scripts de treinamento (ex: `Data-set/lexiconPT-master/lexiconPT-master/data/csv/oplexicon_v3.0.csv`). Verifique as funÃ§Ãµes `load_*_dataset` dentro de cada script `train_*.py` para os caminhos exatos esperados.

### 2. Treinando Modelos

Execute o script de treinamento especÃ­fico para a tarefa desejada. Por exemplo, para treinar o modelo de anÃ¡lise de sentimento:

```bash
python train_sentiment.py --output_dir ./models/sentiment --epochs 3 --batch_size 8 --learning_rate 2e-5
```

Da mesma forma, execute `train_hatbr.py` ou `train_olid.py` para outras tarefas, ajustando os parÃ¢metros conforme necessÃ¡rio:

```bash
python train_hatbr.py --output_dir ./models/hatbr --epochs 5 --batch_size 16
# Ajuste os parÃ¢metros com base na configuraÃ§Ã£o do argparse do script
```

*   Verifique a seÃ§Ã£o `argparse` dentro de cada script (`train_*.py`) para os argumentos de linha de comando disponÃ­veis (como `--output_dir`, `--epochs`, `--batch_size`, `--learning_rate`, etc.).
*   O treinamento salvarÃ¡ o modelo fine-tuned e o tokenizador no diretÃ³rio de saÃ­da especificado.

### 3. Avaliando Modelos

Execute o script de teste correspondente (ex: `test_sentimento.py`, `test_hatbr.py`) para avaliar um modelo treinado. VocÃª provavelmente precisarÃ¡ indicar ao script de teste o diretÃ³rio onde o modelo treinado foi salvo.

*(Exemplo - assumindo que o script de teste recebe o caminho do modelo como argumento)*
```bash
python test_sentimento.py --model_path ./models/sentiment
```
*   Verifique a configuraÃ§Ã£o do `argparse` nos scripts `test_*.py` para uso especÃ­fico.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para mais detalhes. (Assumindo licenÃ§a MIT com base no README anterior)